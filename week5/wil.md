# 인공지능 스터디 5주차 WIL
## 5주차 스터디 목표
5주차의 테마인 Backpropagation과 계산 그래프에 대해 알아보자.
## 5주차 키워드
Backpropagation, 계산 그래프, 연쇄 법칙, 손실함수
### Backpropagation
- Backpropation은 역전파라고 하는데, 인공신경망(Artificial Neural Network)에서 학습을 최적화하는데 사용하는 알고리즘 중 하나라고 한다.  
내가 뽑고자 하는 값(target)과 실제 모델이 계산한 값(output)의 차이를 구하고, 오차값을 뒤로(출력층->입력층) 전파하면서 각 노드(뉴런)가 갖고 있는 변수들을 조정한다.  
- 결과적으로 오차를 점점 줄이며 예측이 정확해진다.  
앞서 배운 다층 퍼셉트론과 딥러닝 모델에서 효과적으로 사용된다.
### 계산 그래프와 연쇄 법칙
- **계산 그래프**: 복잡한 연산을 순차적으로 표현하여, 각 변수의 변화가 최종 출력에 미치는 영향을 시각적으로 확인할 수 있다.  
계산 그래프를 사용하면 각 연산이 신경망에서 차지하는 역할을 명확하게 시각화할 수 있으며, 역전파 시 각 가중치에 대한 미분을 효율적으로 계산할 수 있다.
- **연쇄 법칙**: 미분 연쇄 법칙을 사용해 각 가중치의 기울기를 효율적으로 계산하며, 이를 통해 Backpropagation이 가능해진다.   
예를 들어 \(z\) 함수를 \(x\)에 대해 미분할 때, 중간 변수 \(y\)를 통해 단계별 미분값을 곱하여 최종 미분값을 구할 수 있다.   
이 과정은 특히 딥러닝처럼 여러 층을 가진 네트워크 구조에서 필수적이다.
### 손실 함수와 최적화 방법
- **손실 함수**: 모델의 예측값과 실제 목표값 간의 차이를 계산하는 지표로, 이 값을 최소화하는 것이 학습의 목표다.  
주로 사용되는 손실 함수로는 평균 제곱 오차(MSE)나 교차 엔트로피(Cross-Entropy)가 있다.
- **최적화 방법**: 경사 하강법(Gradient Descent)을 통해 손실값을 최소화하며 학습이 진행된다.  
매 반복마다 손실 함수의 기울기를 계산하여 가중치를 갱신하는 방식으로, 다양한 변형(모멘텀, Adam 등)이 존재한다.
### 역전파
- **복잡한 신경망으로의 확장**: CNN(Convolutional Neural Network)과 같은 복잡한 구조에도 Backpropagation이 적용될 수 있으며, 다수의 가중치와 필터를 효과적으로 학습할 수 있게 해준다. CNN에서는 필터에 대한 기울기도 계산해 가중치와 함께 조정한다.
## 추가학습
- 계산 그래프를 통한 연쇄 법칙의 직관적 이해와 구체적 적용
- CNN과 RNN 같은 다양한 신경망 구조에서의 Backpropagation 적용 방법
- 경사 하강법 변형 기법(모멘텀, Adam 등)과 그 효과 비교

## 느낀점
Backpropagation 알고리즘이 신경망 학습의 핵심이라는 점을 다시 확인할 수 있었고, 계산 그래프를 통해 구체적인 역전파 과정의 이해가 가능했다. 또한, 오차를 최소화하며 학습이 진행되는 과정을 시각적으로 접근하니 더 명확하게 느껴졌다.  
CNN과 같은 복잡한 구조에서도 Backpropagation이 적용된다는 점이 흥미로웠고, 학습 효율성의 중요성을 깨달았다.