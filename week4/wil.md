# 인공지능 스터디 4주차 WIL
## 4주차 스터디 목표
4주차의 테마인 Loss Function과 어떻게 모델이 학습하는지 알아보자.
## 4주차 키워드
Loss Function(손실함수), Regularization(정규화), Optimization(최적화)
### Loss Function
Loss Function에 영향을 주는 요소는 weight matrix와 bias가 있는데, 오늘은 weight matrix에 중점을 두었음  
분류 정확도를 높이기 위한 matrix로, Loss Function 사용 시 Weight Matrix와 실제 정답 간 차이를 수치화 할 수 있음  
#### Loss Function의 종류  
회귀문제에서 사용
- MSE(Mean Squared Error)  
연속형 변수를 예측할 때 사용하며 실제값과 예측값의 차이의 제곱을 계산

분류문제에서 사용
- SVM(Support Vector Machine)  
분류 경계에 가장 가까운 데이터 포인트(SV)를 이용하여 최적의 결정 경계를 찾는 알고리즘    
SVM에서 hinge loss는 Loss Function의 일종으로, 클래스 간 마진을 최대화하여 데이터 포인트가 가장 적합한 클래스에 위치하도록 함  
-마진이란 두 클래스 사이의 경계를 나타내는 선으로부터 가장 가까운 데이터 포인트까지의 거리로, 클수록 강건한 모델임
- CE(Cross Entropy)  
네트워크 출력을 확률로 해석하기 위해 사용  
출력은 보통 실수값으로 나오기 때문에 확률분포로 변환하여 각 클래스에 대한 확신도 확인(Softmax 함수 이용)  
- SVM과 CE의 차이를 보면 SVM은 마진을 중심으로 한 분류 오류에 초점을 맞춘 반면, CE는 모델이 예측한 확률이 실제 클래스와 얼마나 일치하는지에 초점을 둠  
즉, 확률 분포의 차이에 더 민감하게 반응하는 건 CE임
### Regularization
모델이 과적합(Overfitting)되지 않도록 모델의 복잡성을 제어하는 기법  
Overfitting이란 특정 데이터에 너무 맞춰져있어 새로운 데이터에는 적용할 수 없어서 생기는 문제  
L1 Regularization(분류기가 복잡하다고 느껴질 때 쓰는 기법으로, 가중치의 절대값 합을 손실 함수에 추가하는 방식),  
L2 Regularization(매끄러운 그래프를 원할 때 사용하는 기법으로, 가중치의 제곱합을 손실 함수에 추가하여 극단적인 가중치를 피하고, 모든 특성이 영향을 줄 수 있도록 함) 같은 기법을 사용해 해결함
### Optimization
모델이 최적의 성능을 내도록하는 과정으로, 손실함수를 최소화하는 것이 목적임  
다변수함수인 손실함수를 최소화하기 위해 사용하는 알고리즘은 Gradient Descent(경사하강법)으로, 여러 변형이 있음  
기본적으로 원리는 가중치, 편향 등 모델의 매개변수를 조정하는 것임  
현대 많이 쓰는 omptimization은 AdamW임
## 추가학습
가장 많이 사용되고 있다는 AdamW가 궁금해져 추가학습을 할 예정이다.
## 느낀점
모델의 예측값과 실제값의 오차를 측정해 모델이 학습을 통해 오차를 줄이도록 하는  Loss Function에 대해 배웠다. 모델을 구성하는 요소 하나에 대해 배우며 나머지 요소들에 대해서도 잘 공부해보겠다.